{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51bdefb",
   "metadata": {},
   "source": [
    "# Population estimation\n",
    "\n",
    "Obtaining a dynamic population distribution is key to many decision-making processes such as urban planning, disaster management and most importantly helping the government to better allocate socio-technical supply. For the aspiration of these objectives, good population data is essential. In this tutorial, we will...\n",
    "\n",
    "1. Learn about the [Helmholtz AI CountMeIn](#1.-The-Challenge:-Helmholtz-AI-CountMeIn-(April-11-to-May-23,-2022)) challenge,\n",
    "2. Meet [the data](#2.-The-data),\n",
    "3. Explore a [random forest regressor](#3.-Random-forest-regressor)-based solution, and\n",
    "4. Propose a simple [artificial neural network](#4.-Artificial-neural-network)-based solution. Then,\n",
    "5. We will use [data augmentation](#5.-Data-augmentation) to improve the performance, and\n",
    "6. Explore [data fusion](#6.-Data-fusion) to benefit from all data sources. Finally,\n",
    "6. The [open problems](#7.-Open-problems) and future directions will be listed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed72a3c",
   "metadata": {},
   "source": [
    "## 1. The challenge: Helmholtz AI CountMeIn (April 11 to May 23, 2022)\n",
    "\n",
    "[Helmholtz AI](https://www.helmholtz.ai/) is proud to announce an exciting data challenge: the Helmholtz AI CountMeIn Challenge. In this challenge, the participant teams will solve a challenging data science problem from the field of remote sensing: A spatially resolved estimate of the population based on satellite images. The task of the challenge is solving the problem up to a target accuracy with the smallest amount of resources possible. In two tracks, two of the most precious resources are addressed. In track GoGreen, the participant contribution wins that used as little energy. For the GoFast track, the task is to use as little time as possible.\n",
    "\n",
    "This data challenge addresses a scientific audience, and therefore all participants are required to follow Good Scientific Practice. This means that for a valid submission, it is required to make code available, ensure reproducibility and document the steps to solution. The winning contributions to both tracks can present their solutions at the Helmholtz AI Conference.\n",
    "For both tracks, you will be using the HAICORE installations located at Karlsruhe Institute of Technology (KIT) and Forschungszentrum Jülich (FZJ). The submission will happen on the newly formed [Helmholtz Data Challenges Platform](http://helmholtz-data-challenges.de/).\n",
    "\n",
    "Even though the challenge ended, you can submit your results and compare the performance of your solution against the community on the [web](https://helmholtz-data-challenges.de/web/challenges/challenge-page/92/overview)\n",
    "\n",
    "![Data sample](ancillary_rasters_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01c661",
   "metadata": {},
   "source": [
    "## 2. The data\n",
    "\n",
    "Obtaining a dynamic population distribution is key to many decision-making processes such as urban planning, disaster management and most importantly helping the government to better allocate socio-technical supply. For the aspiration of these objectives, good population data is essential. In a view of this, we provide a comprehensive data set for population estimation in 98 European cities. The data set comprises digital elevation model, local climate zone, land use classifications, nighttime lights in combination with multi-spectral Sentinel-2 imagery, and data from the Open Street Map initiative.\n",
    "\n",
    "The final data set consists of two parts, So2Sat POP Part1 and So2Sat POP Part2. All the data patches except OSM data are available as GeoTiff images with the pixel size of 10 m. Along with the raw OSM patches, we also provide the features extracted from the OSM data as the separate Comma Separated Value (CSV) files. So2Sat POP Part1 consists of the patches from local climate zones, land use classification, nighttime lights, Open Street Map and its features, and from all seasons (autumn, summer, spring, winter) of Sentinel-2 imagery (RGB), a total of 1,179,072 files. So2Sat POP Part2 consists of patches from digital elevation model only, a total of 131,008 files. So2Sat POP Part1 has the storage requirement of ~ 38 GB and So2Sat POP Part2 requires ~ 304 MB. Both parts of the data set consists of a predefined train and test split. Out of 98 cities, 78 cities (75% of the data) have been randomly selected as a training set and the rest of the 20 cities (25% of the data) constitute the test set.\n",
    "\n",
    "More details can be found in the original [data set paper](https://arxiv.org/abs/2204.08524). \n",
    "\n",
    "```\n",
    "@article{doda2022so2sat,\n",
    "  title={So2Sat POP--A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale},\n",
    "  author={Doda, Sugandha and Wang, Yuanyuan and Kahl, Matthias and Hoffmann, Eike Jens and Taubenböck, Hannes and Zhu, Xiao Xiang},\n",
    "  journal={arXiv preprint arXiv:2204.08524},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "In this tutorial, we will use a subset of the original data set. Particularly, this subset contains the following data sources:\n",
    "\n",
    "**Population data**: The European Statistical System (ESSnet) project, in co-operation with the European Forum for Geography and Statistics (EFGS), produced the high resolution (1km) population grids from the population census in Europe. We processed this freely available population grids for each city. \n",
    "\n",
    "**Sentinel-2 (SEN2)**: Processed only the RGB bands at 10 m resolution for all four seasonal sets (spring, summer, autumn and winter) of Sentinel-2 images to capture the seasonal variation in the data.\n",
    "\n",
    "**Local climate zones (LCZ)**: Processed the urban local climate zone classifications, So2Sat LCZ v1.0, produced by fusing the freely available satellite data from Sentinel-1 and Sentinel-2 satellites using deep learning.\n",
    "\n",
    "**Nighttime lights (VIIRS)**: Processed the freely available cloud free annual composites of global VIIRS nighttime lights.\n",
    "\n",
    "**Land use classification (LU)**: Mapped OSM tags to a simplified land use classification scheme: commercial, industrial, residential, and other that results in a four band raster with corresponding land use proportions.\n",
    "\n",
    "Train data is taken from Gdansk, while test data comes from Lublin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c370bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c52f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should match the name of the data sets\n",
    "filename_train = 'data/train_countmein.h5'\n",
    "filename_test = 'data/test_countmein.h5'\n",
    "\n",
    "dataset_train = h5py.File(filename_train, 'r')\n",
    "dataset_test = h5py.File(filename_test, 'r')\n",
    "\n",
    "# show the content names\n",
    "print(list(dataset_train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a37067",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_train = np.array(dataset_train['population'])\n",
    "\n",
    "# show the shape\n",
    "print(\"Population train data shape: \", pop_train.shape)\n",
    "\n",
    "# print the ground truth for one sample\n",
    "print(\"This is how the ground truth for one sample looks like:\", pop_train[100,:])\n",
    "\n",
    "# note that it is a pair (class, population count), thus we store just one part\n",
    "pop_count_train = pop_train[:,1]\n",
    "\n",
    "pop_test = np.array(dataset_test['population'])\n",
    "pop_count_test = pop_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Sentinel-2 data (Spring snapshots)\n",
    "sen2_spring_train = np.array(dataset_train['sentinel2_spring'])\n",
    "sen2_spring_test = np.array(dataset_test['sentinel2_spring'])\n",
    "\n",
    "print(\"Sentinel-2 shape: \", sen2_spring_train.shape)\n",
    "# 267, 100x100 pixels, 3 channels samples\n",
    "\n",
    "def norm_img(X):        \n",
    "    return ((X - np.min(X)) / (np.max(X) - np.min(X)))\n",
    "    \n",
    "\n",
    "index = 119\n",
    "\n",
    "plt.imshow(norm_img(sen2_spring_train[index,:]))\n",
    "plt.colorbar()\n",
    "plt.title('Sentinel-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VIIRS\n",
    "viirs_train = np.array(dataset_train['viirs'])\n",
    "\n",
    "print(\"VIIRS shape: \", viirs_train.shape)\n",
    "# 267, 100x100 pixels, 3 channels samples\n",
    "\n",
    "def norm_img(X):        \n",
    "    return ((X - np.min(X)) / (np.max(X) - np.min(X)))\n",
    "    \n",
    "\n",
    "plt.imshow(norm_img(viirs_train[index,:]))\n",
    "plt.colorbar()\n",
    "plt.title('VIIRS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee27bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the population densitiy distribution of the samples\n",
    "plt.hist(pop_count_train, label=\"train\")\n",
    "plt.hist(pop_count_test, label=\"test\")\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba64b23",
   "metadata": {},
   "source": [
    "## 3. Random forest regressor\n",
    "\n",
    "A random forest regressor is a meta estimator that fits a number of classifying regression trees on various sub-samples of the dataset and uses *averaging* to improve the regression performance and control over-fitting.\n",
    "\n",
    "![Random forest regressor](rf-regressor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regressor needs a vector as input\n",
    "_shape = sen2_spring_train.shape\n",
    "X_train = np.reshape(sen2_spring_train, (_shape[0], _shape[1] * _shape[2] * _shape[3]))\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "\n",
    "n_test = sen2_spring_test.shape[0]\n",
    "X_test = np.reshape(sen2_spring_test, (n_test, _shape[1] * _shape[2] * _shape[3]))\n",
    "print(\"X test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee62aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The details of the RandomForestRegressor can be found at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "rf_base = RandomForestRegressor(random_state=3108)\n",
    "\n",
    "rf_base.fit(X_train, pop_count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pred_rf = rf_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563fa51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_base = mean_absolute_error(pop_count_test, pop_pred_rf)\n",
    "rmse_base = mean_squared_error(pop_count_test, pop_pred_rf, squared=False) # squared=True -> MSE, squared=False -> RMSE\n",
    "r2_base = r2_score(pop_count_test, pop_pred_rf)\n",
    "\n",
    "print(\"MAE:\", mae_base) # Best possible is 0\n",
    "print(\"RMSE:\", rmse_base) # Best possible is 0\n",
    "print(\"R2:\", r2_base) # Best possible is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dde22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total population in test set:\", np.sum(pop_count_test))\n",
    "print(\"Total population predicted:\", int(np.sum(pop_pred_rf)))\n",
    "\n",
    "print(\"Average population per sample in test set:\", np.average(pop_count_test))\n",
    "print(\"Average population per sample in predicted set:\", np.average(pop_pred_rf))\n",
    "\n",
    "lim = int(np.min( [np.max(pop_count_test), np.max(pop_pred_rf)]))\n",
    "\n",
    "plt.scatter(pop_count_test, pop_pred_rf, label=\"RF\", color=\"yellow\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Predicted population count\")\n",
    "plt.plot( [0, lim], [0, lim], color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657a0fb",
   "metadata": {},
   "source": [
    "## 4. Artificial neural network\n",
    "\n",
    "The human brain is a *machine* capable of performing very complex tasks, such as pattern recognition, motor control of a limb or perception of sensory stimuli, much faster than any machine invented by man. For this reason, great efforts have been (and continue to be) made to understand how they work. \n",
    "\n",
    "This machine is made up of about 100 trillion neurons (cells of the nervous system specialized in the reception and conduction of stimuli), which communicate with each other, forming complex circuits, which are capable of carrying out brain function. This biological model was the inspiration for McCulloch and Pitts to propose a [new computer model](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf) in 1943: the *artificial neural network* (ANN).\n",
    "\n",
    "An ANN is a network of computer units (neurons) connected to each other, where each unit performs a calculation from an input and communicates its result (output) to the connected units. Typically, the connections are weighted, which indicates how strong the connection between two units is. Also, the calculation units are usually grouped into modules or layers. ANNs have properties and capabilities that are very useful for solving complex problems, highlighting its nonlinearity, adaptability, fault tolerance, among others. Thanks to these properties it has been possible to overcome the results obtained in multiple problems.\n",
    "\n",
    "![Artificial neural network](ann.png)\n",
    "\n",
    "> Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. *Yann LeCun, Yoshua Bengio and Geoffrey Hinton, 2015*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6380f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(_shape[1], _shape[2], _shape[3])),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),  \n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "simple_nn.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2caad0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_nn.fit(sen2_spring_train, pop_count_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pred_nn = simple_nn.predict(sen2_spring_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e66c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_nn = mean_absolute_error(pop_count_test, pop_pred_nn)\n",
    "rmse_nn = mean_squared_error(pop_count_test, pop_pred_nn, squared=False) # squared=True -> MSE, squared=False -> RMSE\n",
    "r2_nn = r2_score(pop_count_test, pop_pred_nn)\n",
    "\n",
    "print(\"MAE:\", mae_nn) # Best possible is 0\n",
    "print(\"RMSE:\", rmse_nn) # Best possible is 0\n",
    "print(\"R2:\", r2_nn) # Best possible is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = int(np.max( [lim, np.max(pop_pred_nn)]))\n",
    "\n",
    "plt.scatter(pop_count_test, pop_pred_nn, label=\"NN\", color=\"blue\")\n",
    "plt.scatter(pop_count_test, pop_pred_rf, label=\"RF\", color=\"yellow\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Predicted population count\")\n",
    "plt.plot( [0, lim], [0, lim], color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6483f92-5e5f-499e-9a81-7b07690141e2",
   "metadata": {},
   "source": [
    "Neural networks are very sensitive to their hyperparameters. Therefore, let us dig a bit into the details..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd2624-27da-4f6e-8277-caed430fd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sen2_spring_train[:,:,:,0].flatten(), label=\"red\", color=\"red\", alpha=0.5)\n",
    "plt.hist(sen2_spring_train[:,:,:,1].flatten(), label=\"green\", color=\"green\", alpha=0.5)\n",
    "plt.hist(sen2_spring_train[:,:,:,2].flatten(), label=\"blue\", color=\"blue\", alpha=0.5)\n",
    "plt.xlabel(\"Pixel value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8e57c-7599-4374-a1a3-456455e5de53",
   "metadata": {},
   "source": [
    "It looks like a good idea to *standarize* the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb5d52-9514-41d0-b731-e29a417816b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_scaler = StandardScaler()\n",
    "s2_scaler.fit(sen2_spring_train.reshape((_shape[0], _shape[1] * _shape[2] * _shape[3])))\n",
    "sen2_spring_train_stdized = s2_scaler.transform(\n",
    "    sen2_spring_train.reshape((_shape[0], _shape[1] * _shape[2] * _shape[3])))\n",
    "sen2_spring_train_stdized = sen2_spring_train_stdized.reshape((_shape[0], _shape[1], _shape[2], _shape[3]))\n",
    "\n",
    "plt.hist(sen2_spring_train_stdized[:,:,:,0].flatten(), label=\"red\", color=\"red\", alpha=0.5)\n",
    "plt.hist(sen2_spring_train_stdized[:,:,:,1].flatten(), label=\"green\", color=\"green\", alpha=0.5)\n",
    "plt.hist(sen2_spring_train_stdized[:,:,:,2].flatten(), label=\"blue\", color=\"blue\", alpha=0.5)\n",
    "plt.xlabel(\"Pixel value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2ffc3-cb8f-4732-b5df-fe121828a8d6",
   "metadata": {},
   "source": [
    "Now, let's normalize the *output*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb45c4-3492-4e8a-bebc-a08095d36136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scaler = MinMaxScaler() \n",
    "pop_scaler.fit(pop_count_train.reshape(-1,1))\n",
    "scaled_pop_count_train = pop_scaler.transform(pop_count_train.reshape(-1,1))\n",
    "\n",
    "plt.hist(scaled_pop_count_train, label=\"train\")\n",
    "plt.xlabel(\"Scaled population count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57578c6-bd44-43ff-8f98-0e8dbb06907a",
   "metadata": {},
   "source": [
    "Let us have a look into the activation functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afcfe6-11da-4cb2-9578-0f18d526b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y_sigmoid = 1 / (1 + np.exp(-x) )\n",
    "y_relu = np.maximum(0, x)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, y_relu)\n",
    "plt.legend(['Relu'])\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, y_sigmoid)\n",
    "plt.legend(['sigmoid function'])\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479257d9-c99b-4edb-9fb6-f9c5598008ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn_v2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(_shape[1], _shape[2], _shape[3])),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid'),  \n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "simple_nn_v2.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error')\n",
    "\n",
    "simple_nn_v2.fit(sen2_spring_train_stdized, scaled_pop_count_train, epochs=5)\n",
    "\n",
    "sen2_spring_test_stdized = s2_scaler.transform(\n",
    "    sen2_spring_test.reshape((n_test, _shape[1] * _shape[2] * _shape[3])))\n",
    "sen2_spring_test_stdized = sen2_spring_test_stdized.reshape((n_test, _shape[1], _shape[2], _shape[3]))\n",
    "\n",
    "pop_pred_nn_scaled_v2 = simple_nn_v2.predict(sen2_spring_test_stdized)\n",
    "pop_pred_nn_v2 = pop_scaler.inverse_transform(pop_pred_nn_scaled_v2)\n",
    "\n",
    "mae_nn_v2 = mean_absolute_error(pop_count_test, pop_pred_nn_v2)\n",
    "rmse_nn_v2 = mean_squared_error(pop_count_test, pop_pred_nn_v2, squared=False) # squared=True -> MSE, squared=False -> RMSE\n",
    "r2_nn_v2 = r2_score(pop_count_test, pop_pred_nn_v2)\n",
    "\n",
    "print(\"MAE:\", mae_nn_v2) # Best possible is 0\n",
    "print(\"RMSE:\", rmse_nn_v2) # Best possible is 0\n",
    "print(\"R2:\", r2_nn_v2) # Best possible is 1\n",
    "\n",
    "lim = int(np.max( [lim, np.max(pop_pred_nn_v2)]))\n",
    "\n",
    "plt.scatter(pop_count_test, pop_pred_nn, label=\"NN\", color=\"blue\")\n",
    "plt.scatter(pop_count_test, pop_pred_nn_v2, label=\"NN v2\", color=\"green\")\n",
    "plt.scatter(pop_count_test, pop_pred_rf, label=\"RF\", color=\"yellow\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Predicted population count\")\n",
    "plt.plot( [0, lim], [0, lim], color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2d2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.hist(pop_pred_nn_scaled_v2, color='green')\n",
    "plt.xlabel(\"Population count scaled\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(pop_pred_nn_v2, color='green')\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc885fc",
   "metadata": {},
   "source": [
    "## 5. Data augmentation\n",
    "\n",
    "ANN (and specially *deep learning* models) are data hungry. Moreover, if not enough training data is given, ANN are prone to overfit the samples. The latter is also related to the *generalization* capability of the model. To overcome these issues, *data augmentation* can be use. Data augmentation consists of increasing the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sample(img, seed=None):\n",
    "    img_flip_lr = tf.image.flip_left_right(img).numpy()\n",
    "    img_flip_ud = tf.image.flip_up_down(img).numpy()\n",
    "    img_bright = tf.image.random_brightness(img, max_delta=0.01, seed=seed).numpy()\n",
    "    img_contrast = tf.image.random_contrast(img, lower=0.2, upper=0.5, seed=seed).numpy()\n",
    "    img_saturation = tf.image.random_saturation(img, lower=5, upper=10, seed=seed).numpy()\n",
    "    img_jpeg = tf.image.random_jpeg_quality(img, min_jpeg_quality=55, max_jpeg_quality=95, seed=seed).numpy()\n",
    "    img_hue = tf.image.random_hue(img, max_delta=0.2, seed=seed).numpy()\n",
    "    return (img,\n",
    "           img_flip_lr,\n",
    "           img_flip_ud,\n",
    "           img_bright,\n",
    "           img_contrast,\n",
    "           img_saturation,\n",
    "           img_jpeg,\n",
    "           img_hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = augment_sample(sen2_spring_train[0], seed=3108)\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(len(aug)):\n",
    "    fig.add_subplot(3, 3, i+1)\n",
    "    plt.imshow(norm_img(aug[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4548b-859c-4f2f-9a99-8c32c891cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = ()\n",
    "tmp_count = np.array(())\n",
    "\n",
    "for i in range(len(sen2_spring_train_stdized)):\n",
    "    aug = augment_sample(sen2_spring_train_stdized[i], seed=3108)\n",
    "    tmp_train = tmp_train + aug\n",
    "    tmp_count = np.hstack((tmp_count, np.repeat(scaled_pop_count_train[i], len(aug))))\n",
    "\n",
    "sen2_spring_aug_train_stdized = np.stack(tmp_train)\n",
    "print(\"Augmented data set shape\", sen2_spring_aug_train_stdized.shape)\n",
    "\n",
    "scaled_pop_count_aug_train = tmp_count.reshape(-1, 1)\n",
    "print(\"Augmented labels shape\", scaled_pop_count_aug_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90573899-6271-4811-9de3-d6f25a3ba98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn_v3 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(_shape[1], _shape[2], _shape[3])),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid'),  \n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "simple_nn_v3.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error')\n",
    "\n",
    "simple_nn_v3.fit(sen2_spring_aug_train_stdized, scaled_pop_count_aug_train, epochs=5)\n",
    "\n",
    "pop_pred_nn_scaled_v3 = simple_nn_v3.predict(sen2_spring_test_stdized)\n",
    "pop_pred_nn_v3 = pop_scaler.inverse_transform(pop_pred_nn_scaled_v3)\n",
    "\n",
    "mae_nn_v3 = mean_absolute_error(pop_count_test, pop_pred_nn_v3)\n",
    "rmse_nn_v3 = mean_squared_error(pop_count_test, pop_pred_nn_v3, squared=False) # squared=True -> MSE, squared=False -> RMSE\n",
    "r2_nn_v3 = r2_score(pop_count_test, pop_pred_nn_v3)\n",
    "\n",
    "print(\"MAE:\", mae_nn_v3) # Best possible is 0\n",
    "print(\"RMSE:\", rmse_nn_v3) # Best possible is 0\n",
    "print(\"R2:\", r2_nn_v3) # Best possible is 1\n",
    "\n",
    "lim = int(np.max( [lim, np.max(pop_pred_nn_v3)]))\n",
    "\n",
    "plt.scatter(pop_count_test, pop_pred_nn, label=\"NN\", color=\"blue\")\n",
    "plt.scatter(pop_count_test, pop_pred_nn_v2, label=\"NN v2\", color=\"green\")\n",
    "plt.scatter(pop_count_test, pop_pred_nn_v3, label=\"NN v3\", color=\"gray\")\n",
    "plt.scatter(pop_count_test, pop_pred_rf, label=\"RF\", color=\"yellow\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Population count\")\n",
    "plt.ylabel(\"Predicted population count\")\n",
    "plt.plot( [0, lim], [0, lim], color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803160a-8cf9-4ca1-a4d6-3be3e7aa0c50",
   "metadata": {},
   "source": [
    "# 6. Data fusion\n",
    "\n",
    "There are several ways of *using* all data sources. Generally speaking, the *data fusion* process can occur at a (1) low level (e.g., concatenate/stack the data patches), (2) feature level (e.g., inside the model), or (3) decision level (e.g., combine the predictions of models that process different data sources).\n",
    "\n",
    "![Data fusion](data_fusion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5fe25-d486-4d47-a677-832bed8be3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76e8763c-86d3-4053-8fea-0d7b2be69af4",
   "metadata": {},
   "source": [
    "# 7. Open problems\n",
    "\n",
    "So far, we have explored just the tip of the iceberg. Particularly, this is an imbalanced problem, and the geographical variability imposes a generalization challenge. Moreover, the multi-modality of the data offers opportunities, but it is challenging to get the most out of it. Generally speaking, remote sensing data bring new challenges for deep learning, since satellite image analysis raises some unique questions that translate into challenging new scientific questions. Particularly, remote sensing data is:\n",
    "- Multi-modal, thus it requires transfering learning from different modalities and (different) sensor fusion.\n",
    "- Geo-located, i.e., each pixel corresponds to a spatial coordinate. Thus, it can be fused with other data sources, and enables new applications, e.g., geo-located services.\n",
    "- Quality controlled measurements, with confidence estimates. Therefore, things like estimating the uncertainty of the prediction made by a DL model become extremely important.\n",
    "- Time-dependent, thus requires tailoring models.\n",
    "- Big, i.e., we are talking about PB of data, and in consequence efficient models are needed. Also, most data is unlabeled, thus learning with few labeled data is critical.\n",
    "- Geo-physical or bio-chemical quantities, so to interpret the data expert knowledge is needed.\n",
    "\n",
    "Besides these challenges, automating the whole process is needed (AutoML) but considering all the particularities; and we must consider the ethical implications of working with Earth observation data!\n",
    "\n",
    "![Open problems](open_problems.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
